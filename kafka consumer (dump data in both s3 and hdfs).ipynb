{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389c5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas kafka-python pymongo s3fs hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91033e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "from time import sleep\n",
    "from json import loads\n",
    "from s3fs import S3FileSystem\n",
    "from hdfs import InsecureClient\n",
    "import json\n",
    "\n",
    "# Kafka configuration\n",
    "bootstrap_servers = '54.208.33.187:9092'  # Replace with your Kafka bootstrap servers\n",
    "topic = 'demo_test'\n",
    "\n",
    "# S3 configuration\n",
    "s3_bucket = 'kafka-aws-ajay-yadav'\n",
    "s3_prefix = 'youtube-comment'\n",
    "s3 = S3FileSystem(anon=True)\n",
    "\n",
    "# HDFS configuration\n",
    "hdfs_namenode = 'spark://localhost:7077'  # Replace with your HDFS namenode\n",
    "hdfs_path = 'hdfs://localhost'\n",
    "hdfs_client = InsecureClient(hdfs_namenode, user='hdfs://localhost')\n",
    "\n",
    "# Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    topic,\n",
    "    bootstrap_servers=[bootstrap_servers],\n",
    "    value_deserializer=lambda x: loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "for count, message in enumerate(consumer):\n",
    "    try:\n",
    "        # Write to S3\n",
    "        s3_key = f'{s3_prefix}/youtube_comment_{count}.json'\n",
    "        with s3.open(f\"s3://{s3_bucket}/{s3_key}\", 'w') as s3_file:\n",
    "            json.dump(message.value, s3_file)\n",
    "\n",
    "        # Write to HDFS\n",
    "        hdfs_file_path = f'hdfs://localhost/youtube_comment_{count}.json'\n",
    "        with hdfs_client.write(hdfs://localhost, encoding='utf-8') as hdfs_file:\n",
    "            json.dump(message.value, hdfs_file)\n",
    "\n",
    "        print(f'Data processed and written to S3 and HDFS: {message.value}')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f'Error processing message: {str(e)}')\n",
    "\n",
    "    sleep(1)  # Sleep for 1 second between processing messages to avoid excessive consumption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5136d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "create a s3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf04eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "create a access key and secreat key on local machine (to send data from local machine to aws\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85475718",
   "metadata": {},
   "outputs": [],
   "source": [
    "download aws cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44d4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "run consumer first and then start the producer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbd2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "you will get the json data (json file for every row converted) in s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f48a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "now go to aws glue and create crawler and that will create a  aws glue  data catalog "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8aab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "then connect to AWS athena for real time  sql querying on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb42cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Python script consumes messages from a Kafka topic ('demo_test'), processes each message,\n",
    "and writes the data to both Amazon S3 and HDFS. It uses the KafkaConsumer to fetch messages,\n",
    "S3FileSystem to interact with Amazon S3, and InsecureClient for HDFS. For each Kafka message received,\n",
    "it writes the data to unique S3 and HDFS locations, \n",
    "with error handling and a 1-second delay between processing messages to avoid excessive consumption. \n",
    "The script essentially acts as a connector, enabling the storage of Kafka messages in distributed storage systems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5e2ba70",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2571802725.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    s3 : Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability,\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# read docs \n",
    "\n",
    "s3 : Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, \n",
    "    data availability, security, and performance. Customers of all sizes and industries can use Amazon S3 to store and\n",
    "    protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and \n",
    "    restore, archive, enterprise applications, IoT devices, and big data analytics. \n",
    "    Amazon S3 provides management features so that you can optimize, organize,\n",
    "    and configure access to your data to meet your specific business, organizational, and compliance requirements.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098f7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aws glue: AWS Glue is a serverless data integration service that makes it easy for analytics users to discover, \n",
    "    prepare, move, and integrate data from multiple sources. You can use it for analytics, machine learning, and \n",
    "    application development. It also includes additional productivity and data ops tooling for authoring, running jobs,\n",
    "    and implementing business workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "glue crawler and glue catalog: The AWS Glue Data Catalog contains references to data that is used as sources and \n",
    "    targets of your extract, \n",
    "    transform, and load (ETL) jobs in AWS Glue. To create your data warehouse or data lake, you must catalog this data. \n",
    "    The AWS Glue Data Catalog is an index to the location, schema, and runtime metrics of your data. You use the information \n",
    "    in the Data Catalog to create and monitor your ETL jobs. Information in the Data Catalog is stored as metadata tables, \n",
    "    where each table specifies a single data store. Typically, you run a crawler to take inventory of the data in your data \n",
    "    stores, but there are other ways to add metadata tables into your Data Catalog. For more information, see AWS Glue tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc2cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "Aws athena: Amazon Athena is an interactive query service that makes it simple to analyze data directly in Amazon S3\n",
    "    using standard SQL. Athena is serverless, so there is no infrastructure to setup or manage, and you can choose\n",
    "    to pay based on the queries you run or compute needed by your queries. Use Athena to process logs, perform data analytics,\n",
    "    and run interactive queries. Athena scales automatically – executing queries in parallel – so results are fast, \n",
    "    even with large datasets and complex queries. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
