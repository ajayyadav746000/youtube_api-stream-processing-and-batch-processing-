{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f30b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ajay yadav\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ajay yadav\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "!pip install apache-airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow initdb #Initialize the Airflow database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23a22a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import psycopg2\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"YouTubeSentimentAnalysis\").getOrCreate()\n",
    "\n",
    "def run_spark_job():\n",
    "    # HDFS file path\n",
    "    hdfs_file_path = 'hdfs://localhost/youtube_comments.csv'  # Replace with your HDFS file path\n",
    "\n",
    "    # Read data from HDFS into a Spark DataFrame\n",
    "    df = spark.read.csv(hdfs_file_path, header=True, inferSchema=True)\n",
    "\n",
    "    # Create a UDF (User Defined Function) for sentiment analysis\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def analyze_sentiment(text):\n",
    "        sentiment_scores = analyzer.polarity_scores(text)\n",
    "        compound_score = sentiment_scores['compound']\n",
    "        if compound_score > 0.05:\n",
    "            return \"Positive\"\n",
    "        elif compound_score < -0.05:\n",
    "            return \"Negative\"\n",
    "        else:\n",
    "            return \"Neutral\"\n",
    "\n",
    "    sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "    # Apply sentiment analysis UDF to the DataFrame\n",
    "    df_with_sentiment = df.withColumn(\"Sentiment\", sentiment_udf(\"cleaned_comment\"))\n",
    "\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    df_pandas = df_with_sentiment.toPandas()\n",
    "\n",
    "    # Write the Pandas DataFrame to a CSV file\n",
    "    csv_output_path = \"/path/to/output21.csv\"\n",
    "    df_pandas.to_csv(csv_output_path, index=False)\n",
    "\n",
    "    # Stop the Spark session\n",
    "    spark.stop()\n",
    "\n",
    "    # Send data to AWS Redshift\n",
    "    redshift_connection_params = {\n",
    "        'dbname': 'your_redshift_db',\n",
    "        'user': 'redshift',\n",
    "        'password': 'redshift',\n",
    "        'host': 'your_redshift_host',\n",
    "        'port': 'your_redshift_port'\n",
    "    }\n",
    "\n",
    "    # Establish connection to Redshift\n",
    "    conn = psycopg2.connect(**redshift_connection_params)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Create Redshift table if not exists\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS youtube_sentiment (\n",
    "        -- Define your table schema based on your DataFrame columns\n",
    "        id VARCHAR,\n",
    "        comment_text VARCHAR,\n",
    "        Sentiment VARCHAR(10)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cursor.execute(create_table_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Copy data from CSV to Redshift\n",
    "    copy_query = f\"\"\"\n",
    "    COPY youtube_sentiment FROM '{csv_output_path}' \n",
    "    CREDENTIALS 'aws_access_key_id=your_access_key_id;aws_secret_access_key=your_secret_access_key'\n",
    "    CSV;\n",
    "    \"\"\"\n",
    "    cursor.execute(copy_query)\n",
    "    conn.commit()\n",
    "\n",
    "    # Close the connection\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    # Print a message\n",
    "    print(\"Data loaded into AWS Redshift\")\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'spark_nlp_v1',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 6, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 5,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'youtube_sentiment_analysis',\n",
    "    default_args=default_args,\n",
    "    description='DAG for YouTube Sentiment Analysis',\n",
    "    schedule_interval='@weekly',  # Run every week on Monday\n",
    ")\n",
    "\n",
    "run_spark_task = PythonOperator(\n",
    "    task_id='run_spark_job',\n",
    "    python_callable=run_spark_job,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dag.cli()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place both your Spark script and the Airflow DAG script in the same directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "592b9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Airflow web server:\n",
    "\n",
    " airflow webserver -p 8080\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24313024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Airflow scheduler in a separate terminal:\n",
    "\n",
    " airflow scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the Airflow web UI at http://localhost:8080 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726fd57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "This script sets up an Apache Airflow DAG for sentiment analysis on YouTube comments. \n",
    "It uses PySpark and NLTK's VADER for analysis, processes data from HDFS, and writes results to both CSV and AWS Redshift. \n",
    "The Airflow DAG is scheduled to run weekly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca91d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
